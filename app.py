import streamlit as st
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
from io import StringIO
import chromedriver_autoinstaller
import subprocess

def setup_chrome_driver():
    # Chrome ì„¤ì¹˜ í™•ì¸ ë° ì„¤ì¹˜
    try:
        subprocess.Popen(
            ['google-chrome', '--version'], 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
    except:
        CHROME_INSTALL_CMD = """
        apt-get update
        apt-get install -y google-chrome-stable
        """
        subprocess.Popen(CHROME_INSTALL_CMD, shell=True)
    
    # ChromeDriver ìë™ ì„¤ì¹˜
    chromedriver_autoinstaller.install()
    
    # Chrome ì˜µì…˜ ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_argument('--ignore-certificate-errors')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    return webdriver.Chrome(options=chrome_options)

def crawl_blog(keyword, num_posts):
    results = []
    log_container = st.empty()
    driver = setup_chrome_driver()
    
    try:
        # ë„¤ì´ë²„ ì ‘ì†
        log_container.write("ë„¤ì´ë²„ì— ì ‘ì† ì¤‘...")
        driver.get("https://www.naver.com")
        driver.maximize_window()
        
        # ê²€ìƒ‰ì–´ ì…ë ¥
        log_container.write(f"ê²€ìƒ‰ì–´ '{keyword}' ì…ë ¥ ì¤‘...")
        search_box = driver.find_element(By.ID, "query")
        search_box.click()
        search_box.send_keys(keyword)
        search_box.send_keys(Keys.RETURN)
        
        # ë¸”ë¡œê·¸ íƒ­ í´ë¦­
        log_container.write("ë¸”ë¡œê·¸ íƒ­ìœ¼ë¡œ ì´ë™ ì¤‘...")
        wait = WebDriverWait(driver, 10)
        blog_tab = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='tab.blog.all']")))
        blog_tab.click()
        time.sleep(3)
        
        # ë¸”ë¡œê·¸ ë§í¬ ìˆ˜ì§‘
        blog_links = driver.find_elements(By.CSS_SELECTOR, "a.title_link")
        log_container.write(f"ì´ {len(blog_links[:num_posts])}ê°œì˜ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤...")
        
        for index, link in enumerate(blog_links[:num_posts], 1):
            blog_data = {
                "ë²ˆí˜¸": index,
                "ì œëª©": link.text,
                "ë§í¬": link.get_attribute('href'),
                "ë³¸ë¬¸": ""
            }
            
            # í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
            if "tistory.com" in blog_data['ë§í¬']:
                log_container.write(f"âš ï¸ [{index}/{num_posts}] í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ëŠ” ê±´ë„ˆëœë‹ˆë‹¤: '{blog_data['ì œëª©']}'")
                continue
            
            log_container.write(f"[{index}/{num_posts}] '{blog_data['ì œëª©']}' í¬ë¡¤ë§ ì¤‘...")
            
            # ìƒˆ íƒ­ì—ì„œ ë¸”ë¡œê·¸ ì—´ê¸°
            driver.execute_script(f"window.open('{blog_data['ë§í¬']}', '_blank')")
            time.sleep(2)
            driver.switch_to.window(driver.window_handles[-1])
            
            try:
                if "post.naver.com" in blog_data['ë§í¬']:
                    log_container.write(f"ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘... ({index}/{num_posts})")
                    content = get_naver_post_content(driver)
                else:
                    log_container.write(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì²˜ë¦¬ ì¤‘... ({index}/{num_posts})")
                    content = get_naver_blog_content(driver)
                    
                blog_data["ë³¸ë¬¸"] = content
                log_container.write(f"âœ… [{index}/{num_posts}] ì™„ë£Œ!")
                results.append(blog_data)
                
            except Exception as e:
                error_msg = f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"
                blog_data["ë³¸ë¬¸"] = error_msg
                log_container.write(f"âŒ [{index}/{num_posts}] ì‹¤íŒ¨: {error_msg}")
                results.append(blog_data)
            
            # íƒ­ ë‹«ê¸°
            driver.close()
            driver.switch_to.window(driver.window_handles[0])
            time.sleep(1)
            
        log_container.write("âœ¨ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
    finally:
        driver.quit()
        
    return pd.DataFrame(results)

def get_naver_post_content(driver):
    # ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ë¡œì§
    time.sleep(3)
    
    try:
        # ì²« ë²ˆì§¸ ë°©ë²•: ê¸°ì¡´ êµ¬ì¡°
        content = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.se_component_wrap"))
        )
        return content.text.strip()
    except:
        try:
            # ë‘ ë²ˆì§¸ ë°©ë²•: ìƒˆë¡œìš´ êµ¬ì¡°
            paragraphs = WebDriverWait(driver, 10).until(
                EC.presence_of_elements_located((By.CSS_SELECTOR, "div.se_component.se_paragraph.default"))
            )
            
            # ëª¨ë“  ë¬¸ë‹¨ì˜ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©
            texts = []
            for p in paragraphs:
                try:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸
                    text_element = p.find_element(By.CSS_SELECTOR, "p.se_textarea")
                    text = text_element.text.strip()
                    if text:
                        texts.append(text)
                except:
                    continue
            
            # ì´ë¯¸ì§€ ìº¡ì…˜ë„ í¬í•¨
            try:
                captions = driver.find_elements(By.CSS_SELECTOR, "span.se_textarea")
                for caption in captions:
                    caption_text = caption.text.strip()
                    if caption_text:
                        texts.append(caption_text)
            except:
                pass
            
            return "\n".join(texts)
            
        except Exception as e:
            raise Exception(f"ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")

def get_naver_blog_content(driver):
    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ë¡œì§
    iframe = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "iframe#mainFrame"))
    )
    driver.switch_to.frame(iframe)
    
    content = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "div.se-main-container"))
    )
    return content.text.strip()

# ë©”ì¸ UI ë¶€ë¶„ ìˆ˜ì •
st.set_page_config(
    page_title="ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title("ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ ğŸ¤–")

with st.expander("â„¹ï¸ ì‚¬ìš© ë°©ë²•", expanded=True):
    st.markdown("""
    1. ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”
    2. í¬ë¡¤ë§í•  ê²Œì‹œë¬¼ ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš” (ìµœëŒ€ 20ê°œ)
    3. 'í¬ë¡¤ë§ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
    4. í¬ë¡¤ë§ì´ ì™„ë£Œë˜ë©´ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    
    âš ï¸ **ì£¼ì˜ì‚¬í•­**
    - í¬ë¡¤ë§ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ì¼ë¶€ ê²Œì‹œë¬¼ì€ ìˆ˜ì§‘ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    """)

# ì‚¬ì´ë“œë°”ì— ì…ë ¥ í¼ ë°°ì¹˜
with st.sidebar:
    st.header("í¬ë¡¤ë§ ì„¤ì •")
    keyword = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", "íŒŒì´ì¬ selenium")
    num_posts = st.number_input("í¬ë¡¤ë§í•  ê²Œì‹œë¬¼ ìˆ˜", 
                               min_value=1, 
                               max_value=20, 
                               value=5,
                               help="í•œ ë²ˆì— ìµœëŒ€ 20ê°œê¹Œì§€ í¬ë¡¤ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    
    start_crawl = st.button("í¬ë¡¤ë§ ì‹œì‘", 
                           help="ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ í¬ë¡¤ë§ì´ ì‹œì‘ë©ë‹ˆë‹¤",
                           type="primary")

# ë©”ì¸ í™”ë©´
if start_crawl:
    try:
        with st.spinner(f"'{keyword}' í‚¤ì›Œë“œë¡œ {num_posts}ê°œì˜ ê²Œì‹œë¬¼ì„ í¬ë¡¤ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            df = crawl_blog(keyword, num_posts)
            
            if len(df) > 0:
                st.success("í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰")
                
                # ê²°ê³¼ í‘œì‹œ
                st.subheader("í¬ë¡¤ë§ ê²°ê³¼")
                st.dataframe(df, use_container_width=True)
                
                # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                csv = df.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=csv,
                    file_name=f"blog_crawling_{keyword}.csv",
                    mime="text/csv",
                    help="í¬ë¡¤ë§ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤"
                )
            else:
                st.warning("í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
                
    except Exception as e:
        st.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        st.info("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.") 