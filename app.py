import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import quote
import re

class NaverBlogCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
        }
        
    def search_blogs(self, keyword, num_posts=1):
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ë° ë°ì´í„° ìˆ˜ì§‘"""
        results = []
        page = 1
        collected = 0
        
        while collected < num_posts:
            # ê²€ìƒ‰ URL ìƒì„± (í˜ì´ì§€ë„¤ì´ì…˜ í¬í•¨)
            encoded_keyword = quote(keyword)
            search_url = f"https://search.naver.com/search.naver?where=blog&query={encoded_keyword}&start={1 + (page-1)*10}"
            
            # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            response = requests.get(search_url, headers=self.headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ëª©ë¡ ì¶”ì¶œ
            blog_items = soup.select(".api_txt_lines.total_tit")
            if not blog_items:  # ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                break
                
            for item in blog_items:
                if collected >= num_posts:
                    break
                    
                blog_data = {
                    "ë²ˆí˜¸": collected + 1,
                    "ì œëª©": item.text.strip(),
                    "ë§í¬": item['href'],
                    "ë³¸ë¬¸": "",
                    "ì‘ì„±ì": "",
                    "ì‘ì„±ì¼": ""
                }
                
                # í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ê±´ë„ˆë›°ê¸°
                if "tistory.com" in blog_data['ë§í¬']:
                    continue
                
                try:
                    # ì‘ì„±ì ì •ë³´ ì¶”ì¶œ
                    writer_elem = item.find_parent().find_parent().select_one(".sub_txt.sub_name")
                    if writer_elem:
                        blog_data["ì‘ì„±ì"] = writer_elem.text.strip()
                    
                    # ì‘ì„±ì¼ ì¶”ì¶œ
                    date_elem = item.find_parent().find_parent().select_one(".sub_txt.sub_date")
                    if date_elem:
                        blog_data["ì‘ì„±ì¼"] = date_elem.text.strip()
                    
                    # ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                    if "post.naver.com" in blog_data['ë§í¬']:
                        content = self.get_naver_post_content(blog_data['ë§í¬'])
                    else:
                        content = self.get_naver_blog_content(blog_data['ë§í¬'])
                        
                    blog_data["ë³¸ë¬¸"] = content
                    results.append(blog_data)
                    collected += 1
                    time.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                    
                except Exception as e:
                    blog_data["ë³¸ë¬¸"] = f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"
                    results.append(blog_data)
                    collected += 1
                    
            page += 1
            time.sleep(1)  # í˜ì´ì§€ ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                    
        return pd.DataFrame(results)
    
    def get_naver_post_content(self, url):
        """ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ"""
        response = requests.get(url, headers=self.headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content_divs = soup.select("div.se_component.se_paragraph.default")
        texts = []
        
        for div in content_divs:
            text = div.get_text(strip=True)
            if text:
                texts.append(text)
                
        return "\n".join(texts)
    
    def get_naver_blog_content(self, url):
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            # ì‹¤ì œ ë¸”ë¡œê·¸ URL ì¶”ì¶œ
            if "blog.naver.com" in url:
                blog_id = re.search(r'blog.naver.com/([^?/]+)', url).group(1)
                log_no = re.search(r'logNo=(\d+)', url).group(1)
                url = f"https://blog.naver.com/PostView.naver?blogId={blog_id}&logNo={log_no}"
                
            response = requests.get(url, headers=self.headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # iframe ë‚´ë¶€ ì»¨í…ì¸  ê°€ì ¸ì˜¤ê¸°
            if "blog.naver.com" in url:
                iframe_url = soup.select_one("#mainFrame")['src']
                if not iframe_url.startswith('http'):
                    iframe_url = f"https://blog.naver.com{iframe_url}"
                response = requests.get(iframe_url, headers=self.headers)
                soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì—¬ëŸ¬ ë²„ì „ì˜ ë¸”ë¡œê·¸ í…œí”Œë¦¿ ì§€ì›
            content = None
            
            # ìµœì‹  ë²„ì „ (se-main-container)
            content = soup.select_one("div.se-main-container")
            if content:
                return content.get_text(strip=True)
                
            # êµ¬ë²„ì „ (post-view)
            content = soup.select_one("div.post-view")
            if content:
                return content.get_text(strip=True)
                
            # ë” ì˜¤ë˜ëœ ë²„ì „
            content = soup.select_one("div#postViewArea")
            if content:
                return content.get_text(strip=True)
                
            return "ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"

# Streamlit UI ì„¤ì •
st.set_page_config(
    page_title="ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title("ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ ğŸ¤–")

with st.expander("â„¹ï¸ ì‚¬ìš© ë°©ë²•", expanded=True):
    st.markdown("""
    1. ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”
    2. í¬ë¡¤ë§í•  ê²Œì‹œë¬¼ ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš” (ìµœëŒ€ 20ê°œ)
    3. 'í¬ë¡¤ë§ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
    4. í¬ë¡¤ë§ì´ ì™„ë£Œë˜ë©´ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    
    âš ï¸ **ì£¼ì˜ì‚¬í•­**
    - í¬ë¡¤ë§ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ì¼ë¶€ ê²Œì‹œë¬¼ì€ ìˆ˜ì§‘ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    """)

# ì‚¬ì´ë“œë°”ì— ì…ë ¥ í¼ ë°°ì¹˜
with st.sidebar:
    st.header("í¬ë¡¤ë§ ì„¤ì •")
    keyword = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", "íŒŒì´ì¬ í¬ë¡¤ë§")
    num_posts = st.number_input(
        "í¬ë¡¤ë§í•  ê²Œì‹œë¬¼ ìˆ˜", 
        min_value=1, 
        max_value=20, 
        value=5,
        help="í•œ ë²ˆì— ìµœëŒ€ 20ê°œê¹Œì§€ í¬ë¡¤ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    
    start_crawl = st.button(
        "í¬ë¡¤ë§ ì‹œì‘", 
        help="ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ í¬ë¡¤ë§ì´ ì‹œì‘ë©ë‹ˆë‹¤",
        type="primary"
    )

# ë©”ì¸ í™”ë©´
if start_crawl:
    try:
        with st.spinner(f"'{keyword}' í‚¤ì›Œë“œë¡œ {num_posts}ê°œì˜ ê²Œì‹œë¬¼ì„ í¬ë¡¤ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            crawler = NaverBlogCrawler()
            df = crawler.search_blogs(keyword, num_posts)
            
            if len(df) > 0:
                st.success("í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰")
                
                # ê²°ê³¼ í‘œì‹œ
                st.subheader("í¬ë¡¤ë§ ê²°ê³¼")
                st.dataframe(df, use_container_width=True)
                
                # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                csv = df.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=csv,
                    file_name=f"blog_crawling_{keyword}.csv",
                    mime="text/csv",
                    help="í¬ë¡¤ë§ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤"
                )
            else:
                st.warning("í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
                
    except Exception as e:
        st.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        st.info("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.") 